{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VTAWaFt7gC6"
      },
      "source": [
        "# Context and Objectives\n",
        "\n",
        "## Context\n",
        "\n",
        "The company provides a feature that allows users to schedule recurring allowances, enabling them to set a specific amount and frequency (e.g., daily, weekly, biweekly, or monthly) for payments they will receive.\n",
        "\n",
        "Recently, issues were identified in the backend process responsible for updating the allowance and payment schedule tables. These issues led to discrepancies, particularly in the `next_payment_day` fields across the datasets. The data provided reflects all recorded events and the state of the backend tables up to **December 3, 2024**, which should be considered as the current date for analysis.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "The objective of this analysis is to investigate the provided datasets and generate a detailed report identifying discrepancies and patterns related to the `next_payment_day` and `payment_date` fields across the backend tables.\n",
        "\n",
        "Key objectives:\n",
        "\n",
        "1. **Identify Discrepancies**:\n",
        "   - Verify if there are any mismatches between the `next_payment_day` from the `allowance_backend_table` and the `payment_date` in the `payment_schedule_backend_table`.\n",
        "   \n",
        "2. **Analyze Data Patterns**:\n",
        "   - Examine recurring issues, such as users having multiple records in the `payment_schedule_backend_table` or incorrect `payment_date` values, and identify potential root causes.\n",
        "   \n",
        "3. **Provide Insights**:\n",
        "   - Highlight potential failures in backend processes that could explain the discrepancies observed.\n",
        "\n",
        "   ### Data Availability\n",
        "The tables in RAW format are available in the repository, allowing for direct analysis and verification of the discrepancies observed\n",
        "https://gist.github.com/DaniModak/d0cdc441bc2cab2abdc5b37e45ca5cb4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWz-0HcbTS6A"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import os #For interacting with the operating system, such as accessing file directories\n",
        "import json #For reading data from the JSON file containing the events.\n",
        "import pandas as pd #For data manipulation and analysis, converting JSON/CSV data into DataFrames.\n",
        "from datetime import datetime, timedelta #For manipulating dates and time intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI2EjZ21YlIM"
      },
      "source": [
        "# \"Overview of Functions Used in the Code\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLEPg69vVPo-"
      },
      "source": [
        "The `load_initial_file()` function loads a JSON or CSV file into a Pandas DataFrame:  \n",
        "- It checks if the file exists. If not, it displays an error message.  \n",
        "- For JSON, it uses `json.load()` and normalizes it with `pd.json_normalize()`.  \n",
        "- For CSV, it uses `pd.read_csv()`.  \n",
        "- It handles exceptions with `try...except`, displaying read errors.  \n",
        "- It prints the first few rows of the DataFrame and returns it, or `None` if there is an error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waSctvjUXWYc"
      },
      "outputs": [],
      "source": [
        "def load_initial_file(file_path, file_type=\"json\"):\n",
        "    \"\"\"\n",
        "    Loads a JSON or CSV file into a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Full path to the file.\n",
        "        file_type (str): Type of file: 'json' or 'csv'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame loaded from the file.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if file_type == \"json\":\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            df = pd.json_normalize(data)\n",
        "        elif file_type == \"csv\":\n",
        "            df = pd.read_csv(file_path)\n",
        "        else:\n",
        "            print(f\"Unknown file type: {file_type}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nDataFrame from {file_type.upper()} ({file_path}):\")\n",
        "        print(df.head())\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file {file_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqGY4dNYg3E"
      },
      "source": [
        "\n",
        "The `calculate_next_ocurrence` function calculates the next valid date based on a given starting date, frequency type (weekly, biweekly, daily, or monthly), and a target day. The function uses these inputs to determine the next occurrence of the specified frequency and day, iterating until the calculated date reaches or exceeds a predefined limit (December 3, 2024).\n",
        "\n",
        "#### Parameters:\n",
        "- **`start_date`** (`str`, `datetime`, `pd.Timestamp`): The initial date from which the calculation begins. It can be in various formats (e.g., string, datetime object, or pandas Timestamp).\n",
        "- **`frequency_type`** (`str`): The frequency type for the occurrence. This can be one of the following:\n",
        "  - `'weekly'`: For weekly occurrences.\n",
        "  - `'biweekly'`: For biweekly occurrences.\n",
        "  - `'daily'`: For daily occurrences.\n",
        "  - `'monthly'`: For monthly occurrences.\n",
        "- **`target_day`** (`str`): The target day for the occurrence. This can be:\n",
        "  - A weekday name (e.g., 'Monday', 'Tuesday') for weekly or biweekly frequencies.\n",
        "  - `'first_day'`: For the first day of the next month.\n",
        "  - `'fifteenth_day'`: For the fifteenth day of the next month.\n",
        "\n",
        "#### Returns:\n",
        "- **`str`**: The day of the next occurrence in the format `dd` (day of the month).\n",
        "\n",
        "#### Function Behavior:\n",
        "- **Weekly and Biweekly Frequencies**: The function calculates the next occurrence of the specified weekday (e.g., if `target_day` is 'Monday', it finds the next Monday after the `start_date`). For biweekly occurrences, it skips one additional week before the next occurrence.\n",
        "- **Daily Frequency**: The function calculates the next day after the `start_date`.\n",
        "- **Monthly Frequency**: If `target_day` is `'first_day'`, it returns the first day of the next month. If `target_day` is `'fifteenth_day'`, it returns the fifteenth day of the next month. If the `start_date` already passes the target day, it will calculate the occurrence for the next month.\n",
        "  \n",
        "The calculation stops when the result exceeds or matches the predefined limit of **December 3, 2024**, ensuring that no occurrence surpasses this date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snH3fiSqWDnj"
      },
      "outputs": [],
      "source": [
        "def calculate_next_ocurrence(start_date, frequency_type, target_day):\n",
        "    \"\"\"\n",
        "    The function uses these inputs to determine the next occurrence of the specified frequency and day,\n",
        "    iterating until the calculated date reaches or exceeds a predefined limit (December 3, 2024).\n",
        "\n",
        "    Args:\n",
        "        start_date (str, datetime, or pd.Timestamp): The initial date from which calculations will begin.\n",
        "        frequency_type (str): The frequency type for the calculation: 'weekly', 'biweekly', 'daily', or 'monthly'.\n",
        "        target_day (str): The target day for the calculation. For 'weekly' and 'biweekly', provide the day of the week (e.g., 'monday', 'sunday').\n",
        "                           For 'monthly', use 'first_day' or 'fifteenth_day'.\n",
        "\n",
        "    Returns:\n",
        "        str: The last calculated date in the format 'dd', or None if an invalid input is encountered.\n",
        "    \"\"\"\n",
        "\n",
        "    # If start_date is NaT or null, return None to ignore\n",
        "    if pd.isna(start_date) or start_date == \"NaT\":\n",
        "        return None\n",
        "\n",
        "    # If it's already a datetime, do nothing\n",
        "    if isinstance(start_date, datetime):\n",
        "        pass\n",
        "    # If it's a Timestamp, convert to datetime\n",
        "    elif isinstance(start_date, pd.Timestamp):\n",
        "        start_date = start_date.to_pydatetime()\n",
        "    else:\n",
        "        try:\n",
        "            # Remove milliseconds if present\n",
        "            start_date = start_date.split('.')[0]\n",
        "            # Try to convert to datetime with time\n",
        "            start_date = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
        "        except ValueError:\n",
        "            # If it fails, try the format without time\n",
        "            start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
        "\n",
        "    # Define the limit date as December 3, 2024\n",
        "    limit_date = datetime(2024, 12, 3)\n",
        "    result_date = None\n",
        "\n",
        "    while True:\n",
        "        if frequency_type == \"weekly\" or frequency_type == \"biweekly\":\n",
        "            if frequency_type == 'biweekly':\n",
        "                weekly_increment = 2\n",
        "            elif frequency_type == 'weekly':\n",
        "                weekly_increment = 1\n",
        "\n",
        "            week_days_map = {\n",
        "                \"sunday\": 6,\n",
        "                \"monday\": 0,\n",
        "                \"tuesday\": 1,\n",
        "                \"wednesday\": 2,\n",
        "                \"thursday\": 3,\n",
        "                \"friday\": 4,\n",
        "                \"saturday\": 5\n",
        "            }\n",
        "\n",
        "            if target_day not in week_days_map:\n",
        "                return \"Invalid day for weekly or biweekly\"\n",
        "\n",
        "            day_of_week = week_days_map[target_day]\n",
        "\n",
        "            # Calculate the next desired day in the week\n",
        "            days_until_next = (day_of_week - start_date.weekday()) % 7\n",
        "            if days_until_next == 0:\n",
        "                days_until_next = 7 * weekly_increment\n",
        "            else:\n",
        "                days_until_next += 7 * (weekly_increment - 1)\n",
        "\n",
        "            result_date = start_date + timedelta(days=days_until_next)\n",
        "\n",
        "        elif frequency_type == \"daily\":\n",
        "            # For daily, just add one day\n",
        "            result_date = start_date + timedelta(days=1)\n",
        "\n",
        "        elif frequency_type == \"monthly\":\n",
        "            # If frequency is monthly, adjust based on the day\n",
        "            if target_day == \"first_day\":\n",
        "                # If it's \"first day\", go to the first day of the next month\n",
        "                if start_date.month == 12:\n",
        "                    result_date = datetime(start_date.year + 1, 1, 1)\n",
        "                else:\n",
        "                    result_date = datetime(start_date.year, start_date.month + 1, 1)\n",
        "\n",
        "            elif target_day == \"fifteenth_day\":\n",
        "                # If it's \"fifteenth day\", check the current date\n",
        "                if start_date.day < 15:\n",
        "                    # If before or on the 15th, go to the 15th of the same month\n",
        "                    result_date = datetime(start_date.year, start_date.month, 15)\n",
        "                else:\n",
        "                    # If after the 15th, go to the 15th of the next month\n",
        "                    if start_date.month == 12:\n",
        "                        result_date = datetime(start_date.year + 1, 1, 15)\n",
        "                    else:\n",
        "                        result_date = datetime(start_date.year, start_date.month + 1, 15)\n",
        "            else:\n",
        "                return \"Invalid day for monthly\"\n",
        "\n",
        "        else:\n",
        "            return \"Invalid frequency type\"\n",
        "\n",
        "        # Update the start_date for the next calculation\n",
        "        start_date = result_date\n",
        "\n",
        "        # Check if the limit date has been reached\n",
        "        if result_date > limit_date:\n",
        "            break\n",
        "\n",
        "    # If the last date was on or before the limit, calculate once more\n",
        "    if result_date <= limit_date:\n",
        "        if frequency_type == \"weekly\" or frequency_type == \"biweekly\":\n",
        "            result_date += timedelta(weeks=weekly_increment)\n",
        "        elif frequency_type == \"daily\":\n",
        "            result_date += timedelta(days=1)\n",
        "        elif frequency_type == \"monthly\":\n",
        "            if target_day == \"first_day\":\n",
        "                if result_date.month == 12:\n",
        "                    result_date = datetime(result_date.year + 1, 1, 1)\n",
        "                else:\n",
        "                    result_date = datetime(result_date.year, result_date.month + 1, 1)\n",
        "            elif target_day == \"fifteenth_day\":\n",
        "                if result_date.month == 12:\n",
        "                    result_date = datetime(result_date.year + 1, 1, 15)\n",
        "                else:\n",
        "                    result_date = datetime(result_date.year, result_date.month + 1, 15)\n",
        "\n",
        "    return result_date.strftime(\"%d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjlatbDsdY4y"
      },
      "source": [
        "`calculate_incremented_date`: Calculates the next exact date based on the given start date, frequency type (daily, weekly, biweekly, or monthly), and target day. It does not involve any looping but directly computes the next occurrence of the specified date, according to the provided parameters.\n",
        "\n",
        "#### Arguments:\n",
        "- **`data_inicial`** (`datetime` or `str`): The initial date to start the calculation. Can be in various formats.\n",
        "- **`tipo_frequencia`** (`str`): The frequency type. Can be `\"daily\"`, `\"weekly\"`, `\"biweekly\"`, or `\"monthly\"`.\n",
        "- **`dia`** (`str`): The target day for the frequency. Can be a weekday (e.g., `\"monday\"`, `\"sunday\"`) or specific days like `\"first_day\"` or `\"fifteenth_day\"` for monthly frequency.\n",
        "\n",
        "#### Returns:\n",
        "- **`str`**: The resulting day of the month in `dd` format.\n",
        "- If the date can't be calculated (due to invalid parameters), returns an error message (e.g., `\"Invalid day for weekly or biweekly\"`).\n",
        "\n",
        "#### Function Workflow:\n",
        "1. **Initial Date Conversion**:\n",
        "   - Converts the `data_inicial` into a `datetime` object if it's in string format or a `Timestamp`.\n",
        "2. **Date Calculation**:\n",
        "   - Based on the `tipo_frequencia`, calculates the next valid occurrence of the target day:\n",
        "     - **Weekly**: Finds the next occurrence of the specified weekday.\n",
        "     - **Biweekly**: Finds the next occurrence of the specified weekday, skipping one full week.\n",
        "     - **Daily**: Adds exactly one day to the start date.\n",
        "     - **Monthly**: Calculates the next occurrence of the specified day, either the first day or the fifteenth day of the next month.\n",
        "3. **No Loops**: The function directly calculates the next date based on the given parameters, without iteration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_xn_HwTePNE"
      },
      "outputs": [],
      "source": [
        "def calculate_incremented_date(start_date, frequency_type, day):\n",
        "    \"\"\"\n",
        "    Calculates the next exact occurrence based on the provided start date, frequency type, and target day.\n",
        "    This function does not use loops. It directly calculates the next valid date based on the given frequency\n",
        "    and target day.\n",
        "\n",
        "    Args:\n",
        "        start_date (datetime or str): The initial date from which to calculate the next occurrence.\n",
        "        frequency_type (str): The frequency type, which can be 'weekly', 'biweekly', 'daily', or 'monthly'.\n",
        "        day (str): The target day to calculate the next occurrence. For 'weekly' or 'biweekly', this is a day of the week (e.g., 'monday').\n",
        "                   For 'monthly', it can be 'first_day' or 'fifteenth_day'.\n",
        "\n",
        "    Returns:\n",
        "        str: The day of the month for the next occurrence in the format 'dd'.\n",
        "        If the calculation fails or the frequency type is invalid, returns an error message.\n",
        "    \"\"\"\n",
        "\n",
        "    # If start_date is NaT or null, return None to ignore\n",
        "    if pd.isna(start_date) or start_date == \"NaT\":\n",
        "        return None\n",
        "\n",
        "    # If already a datetime, do nothing\n",
        "    if isinstance(start_date, datetime):\n",
        "        pass\n",
        "    # If it's a Timestamp, convert to datetime\n",
        "    elif isinstance(start_date, pd.Timestamp):\n",
        "        start_date = start_date.to_pydatetime()\n",
        "    else:\n",
        "        try:\n",
        "            # Remove milliseconds if any\n",
        "            start_date = start_date.split('.')[0]\n",
        "            # Try to convert to the format with time\n",
        "            start_date = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
        "        except ValueError:\n",
        "            # If it fails, try the format without time\n",
        "            start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
        "\n",
        "    result_date = None\n",
        "\n",
        "    # Handle the frequency calculation\n",
        "    if frequency_type == \"weekly\" or frequency_type == \"biweekly\":\n",
        "        weekly_increment = 1 if frequency_type == \"weekly\" else 2\n",
        "\n",
        "        week_days = {\n",
        "            \"sunday\": 6,\n",
        "            \"monday\": 0,\n",
        "            \"tuesday\": 1,\n",
        "            \"wednesday\": 2,\n",
        "            \"thursday\": 3,\n",
        "            \"friday\": 4,\n",
        "            \"saturday\": 5\n",
        "        }\n",
        "\n",
        "        if day not in week_days:\n",
        "            return \"Invalid day for weekly or biweekly\"\n",
        "\n",
        "        day_of_week = week_days[day]\n",
        "\n",
        "        # Calculate the next desired day in the week\n",
        "        days_until_next = (day_of_week - start_date.weekday()) % 7\n",
        "        if days_until_next == 0:\n",
        "            days_until_next = 7 * weekly_increment\n",
        "        else:\n",
        "            days_until_next += 7 * (weekly_increment - 1)\n",
        "\n",
        "        result_date = start_date + timedelta(days=days_until_next)\n",
        "\n",
        "    elif frequency_type == \"daily\":\n",
        "        # For daily, just add one day\n",
        "        result_date = start_date + timedelta(days=1)\n",
        "\n",
        "    elif frequency_type == \"monthly\":\n",
        "        # If the frequency is monthly, adjust based on the day\n",
        "        if day == \"first_day\":\n",
        "            # If \"first day\", go to the first day of the next month\n",
        "            if start_date.month == 12:\n",
        "                result_date = datetime(start_date.year + 1, 1, 1)\n",
        "            else:\n",
        "                result_date = datetime(start_date.year, start_date.month + 1, 1)\n",
        "\n",
        "        elif day == \"fifteenth_day\":\n",
        "            # If \"fifteenth day\", check the current date\n",
        "            if start_date.day < 15:\n",
        "                # If before or on the 15th, go to the 15th of the same month\n",
        "                result_date = datetime(start_date.year, start_date.month, 15)\n",
        "            else:\n",
        "                # If after the 15th, go to the 15th of the next month\n",
        "                if start_date.month == 12:\n",
        "                    result_date = datetime(start_date.year + 1, 1, 15)\n",
        "                else:\n",
        "                    result_date = datetime(start_date.year, start_date.month + 1, 15)\n",
        "        else:\n",
        "            return \"Invalid day for monthly\"\n",
        "\n",
        "    else:\n",
        "        return \"Invalid frequency type\"\n",
        "\n",
        "    # Return the date formatted, if it was correctly calculated\n",
        "    if result_date is not None:\n",
        "        return result_date.strftime(\"%d\")\n",
        "    else:\n",
        "        return \"Error in date calculation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIUkrDu_pn0B"
      },
      "source": [
        "The function `categorize_discrepancy` categorizes discrepancies based on the difference between two timestamps. If the absolute value of the timestamp difference is less than 1 day, the function classifies the discrepancy as `'backend logic issues'`. Otherwise, it categorizes it as `'timestamp delay issue'`. This helps to distinguish between logic errors and delays in the timestamp processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8GJVP9jpmzL"
      },
      "outputs": [],
      "source": [
        "def categorize_discrepancy(row):\n",
        "\n",
        "    \"\"\"\n",
        "    Categorizes discrepancies based on the difference between two timestamps, `timestamp_diff`.\n",
        "    - If the absolute value of `timestamp_diff` is less than 1 day, classify as 'backend logic issues'.\n",
        "    - Otherwise, classify as 'timestamp delay issue'.\n",
        "\n",
        "    Parameters:\n",
        "    row (pandas.Series): A row of the DataFrame containing the timestamp_diff value.\n",
        "\n",
        "    Returns:\n",
        "    str: The category of the discrepancy ('backend logic issues' or 'timestamp delay issue').\n",
        "    \"\"\"\n",
        "\n",
        "    # If the timestamp_diff is less than 1 day, categorize as 'backend logic issues'\n",
        "    if abs(row['timestamp_diff']) < pd.Timedelta(days=1):\n",
        "        return 'backend logic issues'\n",
        "    else:\n",
        "        return 'timestamp delay issue'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY-u2J6FfqPW"
      },
      "source": [
        "# Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5atcaVvMgRnH"
      },
      "source": [
        "Starting using the funtion to load the files for analysys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "relG2yQ6gkqo",
        "outputId": "9496819c-12bf-4470-b8c7-06e352d77acd"
      },
      "outputs": [],
      "source": [
        "# Get the directory where the script is located\n",
        "file_path = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "# Build paths for the files\n",
        "allowance_events_path = os.path.join(file_path, \"allowance_events.json\")\n",
        "allowance_backend_path = os.path.join(file_path, \"allowance_backend_table.csv\")\n",
        "payment_schedule_path = os.path.join(file_path, \"payment_schedule_backend_table.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV5rTAjjhgeV"
      },
      "outputs": [],
      "source": [
        "# Load files using the generic function\n",
        "allowance_events_df = load_initial_file(allowance_events_path, \"json\")\n",
        "allowance_backend_df = load_initial_file(allowance_backend_path, \"csv\")\n",
        "payment_schedule_df = load_initial_file(payment_schedule_path, \"csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5R3NoHDiqU3"
      },
      "source": [
        "\n",
        "In this section, we ensure that the data types in the three DataFrames (`allowance_events_df`, `allowance_backend_df`, `payment_schedule_df`) are correctly defined:\n",
        "\n",
        "- **`df_events`**: Converts columns like `'event.name'`, `'user.id'`, and `'allowance.amount'` to appropriate types (`category`, `string`, `float`) and transforms the `'event.timestamp'` to `datetime`.\n",
        "  \n",
        "- **`df_backend`**: Converts columns such as `'uuid'` and `'status'` to the correct types and processes `'updated_at'` by converting it to `datetime`, handling missing values and Unix timestamps.\n",
        "\n",
        "- **`df_payment`**: Converts `'user_id'` to `string` and `'payment_date'` to `int`.\n",
        "\n",
        "These transformations ensure consistent data types for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtX-ELagi14m"
      },
      "outputs": [],
      "source": [
        "# Convert data types for allowance_events_df to ensure correct data format\n",
        "df_events = allowance_events_df.astype({\n",
        "    'event.name': 'category',\n",
        "    'user.id': 'string',\n",
        "    'allowance.amount': 'float',\n",
        "    'allowance.scheduled.frequency': 'category',\n",
        "    'allowance.scheduled.day': 'string'\n",
        "})\n",
        "\n",
        "# Convert 'event.timestamp' to datetime format\n",
        "df_events['event.timestamp'] = pd.to_datetime(allowance_events_df['event.timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "#Print to check if the data was correctly hadled\n",
        "print(df_events.dtypes)\n",
        "print(df_events.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAms0AMPji_K"
      },
      "outputs": [],
      "source": [
        "# Convert data types for allowance_backend_df to ensure correct data format\n",
        "df_backend = allowance_backend_df.astype({\n",
        "    'uuid': 'string',\n",
        "    'creation_date': 'string',\n",
        "    'frequency': 'category',\n",
        "    'day': 'string',\n",
        "    'next_payment_day': 'int',\n",
        "    'status': 'category'\n",
        "})\n",
        "\n",
        "# Convert 'updated_at' to datetime format and remove timezone information\n",
        "df_backend['updated_at'] = pd.to_datetime(allowance_backend_df['updated_at'], errors='coerce').dt.tz_localize(None)\n",
        "\n",
        "# Check where conversion failed (values as 'NaT')\n",
        "print(df_backend[df_backend['updated_at'].isna()])\n",
        "\n",
        "# Convert Unix timestamp values to datetime and remove timezone as well\n",
        "df_backend['updated_at'] = df_backend['updated_at'].fillna(\n",
        "    pd.to_datetime(allowance_backend_df['updated_at'], unit='s', errors='coerce').dt.tz_localize(None)\n",
        ")\n",
        "\n",
        "# Verify the data types in the backend DataFrame and display the first few rows\n",
        "print(df_backend.dtypes)\n",
        "print(df_backend.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JnIfhMHkwtZ"
      },
      "outputs": [],
      "source": [
        "# Convert data types for payment_schedule_df to ensure correct data format\n",
        "df_payment = payment_schedule_df.astype({\n",
        "    'user_id': 'string',\n",
        "    'payment_date': 'int'\n",
        "})\n",
        "\n",
        "# Verify the data types in the backend DataFrame and display the first few rows\n",
        "print(df_payment.dtypes)\n",
        "print(df_payment.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXorJUIHo2J1"
      },
      "source": [
        "#Analysis of Disabled and Duplicate Users in Events and Backend Tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19K08Q5Mo5rV"
      },
      "source": [
        "In this section, we analyze the presence of disabled and duplicate users in the `df_events` and `df_backend` tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEK7LFXgpFVd"
      },
      "outputs": [],
      "source": [
        "#Count users with 'disabled' status in backend table\n",
        "disabled_users_backend = df_backend[df_backend[\"status\"] == \"disabled\"]\n",
        "num_disabled_users_backend = disabled_users_backend.shape[0]\n",
        "\n",
        "# Display the number of users with 'disabled' status\n",
        "print(f'Number of users with \"disabled\" status in the allowance_backend_table: {num_disabled_users_backend}')\n",
        "\n",
        "# Calculate the percentage of disabled users\n",
        "total_users_backend = df_backend.shape[0]\n",
        "percentage_disabled_users = (num_disabled_users_backend / total_users_backend) * 100\n",
        "\n",
        "# Step 2: Print the users with 'disabled' status in allowance_backend_table\n",
        "print(\"List of disabled users in the allowance_backend_table:\")\n",
        "print(disabled_users_backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5avTi7hq8cM"
      },
      "source": [
        "Now, we need to remove these users from allowance_backend_table (`df_backend`) making sure that we will only use enables allowances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4G5QPYbrUq8"
      },
      "outputs": [],
      "source": [
        "# Step 2: Remove users with 'disabled' status from the dataframe\n",
        "df_backend_enabled_users = df_backend[df_backend[\"status\"] != \"disabled\"].reset_index(drop=True)\n",
        "\n",
        "# Display the dataframe without the disabled users\n",
        "print(df_backend_enabled_users.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik0YnlRzruSy"
      },
      "source": [
        "Now, let's analyze and remove the disabled users from the `df_events` dataframe to ensure consistency in the analysis by eliminating discrepancies caused by the presence of these users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jhTWYUttEwX"
      },
      "source": [
        "First, we count the number of disabled users present in the `df_events` (allowance_events_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToembATksBDd"
      },
      "outputs": [],
      "source": [
        "# Merge the df_backend and df_events tables\n",
        "df_merged = pd.merge(df_backend, df_events, left_on='uuid', right_on='user.id', how='inner')\n",
        "\n",
        "# Filter users with 'disabled' status in the df_merged table\n",
        "disabled_users_events = df_merged[df_merged[\"status\"] == \"disabled\"]\n",
        "\n",
        "# Count the number of disabled users present in the events table\n",
        "num_disabled_users_events = disabled_users_events[\"user.id\"].nunique()\n",
        "\n",
        "# Display the number of disabled users present in the events table\n",
        "print(f'Number of disabled users present in the events table: {num_disabled_users_events}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1VxMSretUUl"
      },
      "source": [
        "and then we remove these users from `df_events`(allowance_events_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPLA5p3HtaTa"
      },
      "outputs": [],
      "source": [
        "#Get the user IDs of disabled users present in the events table\n",
        "disabled_user_ids_in_events = disabled_users_events[\"user.id\"].unique()\n",
        "\n",
        "#Remove these users from the df_events table and reset the index of the resulting dataframe to ensure a clean, sequential index after filtering\n",
        "df_events_cleaned = df_events[~df_events[\"user.id\"].isin(disabled_user_ids_in_events)].reset_index(drop=True)\n",
        "\n",
        "# Checking number of users before removal\n",
        "num_users_before = len(df_events)\n",
        "\n",
        "# Checking number of users after removing the disabled ones\n",
        "num_users_after = len(df_events_cleaned)\n",
        "\n",
        "#Calculationg number of users removed\n",
        "num_users_removed = num_users_before - num_users_after\n",
        "\n",
        "# Display the number of removed users\n",
        "# The number of users removed in the df_events table is higher because a single user can be associated with multiple events.\n",
        "print(f\"Number of users removed: {num_users_removed}\")\n",
        "\n",
        "# Display the cleaned dataframe\n",
        "print(f\"The cleaned dataframe after removing disabled users is:\")\n",
        "print(df_events_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVZ_JKmDlQX5"
      },
      "source": [
        "# Comparative Analysis: Event-Based Payment Dates vs. Backend System Dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJGkOaTDltMP"
      },
      "source": [
        "In this section, we will calculate the correct payment date based on the event data table (considered the source of truth). We will then compare these calculated payment dates with the dates in the backend system. If the dates match, it will indicate that the backend system is accurate. If there is a discrepancy, further analysis will be conducted to identify the root cause of the error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNnOO-WkmVAQ"
      },
      "source": [
        "In first step, we will calculate the correct payment date based on the `event.timestamp` and `allowance.scheduled.frequency` fields from the cleaned events table (without disabled users), along with the `allowance.scheduled.day`. We will use the `calculate_next_occurrence` function to determine the next correct payment date according to the specified frequency and timestamp.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV-BEpCLnKcl"
      },
      "outputs": [],
      "source": [
        "# Calculating the 'next_correct_allowance_day' column and adding it to df_events_cleaned\n",
        "df_events_cleaned[\"next_expected_payment_date\"] = df_events_cleaned.apply(\n",
        "    lambda row: calculate_next_ocurrence(\n",
        "        row[\"event.timestamp\"],\n",
        "        row[\"allowance.scheduled.frequency\"],\n",
        "        row[\"allowance.scheduled.day\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Displaying the results with new column\n",
        "print(df_events_cleaned[[\"user.id\", \"event.timestamp\", \"next_expected_payment_date\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ENMZ0PwD40"
      },
      "source": [
        "It is essential to retain only the most recent modification `event.timestamp` for each user in `df_events_cleaned`to simplify the comparison process. In the backend table, we store only the latest record for each usery, unlike the events table. So, as the next step, I will sort the data and retain only the most recent record for each user in `df_events_cleaned`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YSbWzcFxTYy"
      },
      "outputs": [],
      "source": [
        "##### Filtering to keep only the latest entry per user #####\n",
        "\n",
        "# Sort by user.id and event.timestamp to ensure the latest event for each user is at the top\n",
        "df_events_cleaned = df_events_cleaned.sort_values(by=[\"user.id\", \"event.timestamp\"], ascending=[True, False])\n",
        "\n",
        "# Remove duplicates, keeping only the latest event for each user\n",
        "df_events_cleaned = df_events_cleaned.drop_duplicates(subset=\"user.id\", keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "# Display the filtered dataframe\n",
        "print(\"Filtered allowance_ events dataFrame with the latest event per user:\")\n",
        "print(df_events_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbT2fk-LvVYc"
      },
      "source": [
        "Now, with only the latest records, I can begin comparing the `next_expected_payment_date` from the `df_events_cleaned`, which was calculated using the `calculate_next_occurrence` function, with the `next_payment_day` field from the `allowance_backend` table.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tuwQosq0_6u"
      },
      "source": [
        "First, we do the merge between the two dataframes to bring the fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTOrNXF6z8Op"
      },
      "outputs": [],
      "source": [
        "# Merging df_events_cleaned with df_backend\n",
        "df_events_merged = pd.merge(\n",
        "    df_events_cleaned,\n",
        "    df_backend[['uuid', 'creation_date', 'frequency', 'day', 'updated_at', 'next_payment_day']],\n",
        "    left_on='user.id',    # Key from df_events_cleaned\n",
        "    right_on='uuid',      # Key from df_backend\n",
        "    how='left'            # To keep all events, even those without a match in the backend\n",
        ")\n",
        "\n",
        "# Displaying the final result with the relevant columns\n",
        "print(\"Filtered allowance_ events dataFrame with the latest event per user:\")\n",
        "print(df_events_merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWhYcZ3L1HQg"
      },
      "source": [
        "To handle data type differences, we convert next_payment_day to remove the decimal part:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkk0ITds1leA"
      },
      "outputs": [],
      "source": [
        "df_events_merged['next_payment_day'] = df_events_merged['next_payment_day'].fillna(0).astype(int).astype(str).str.zfill(2)\n",
        "\n",
        "# Displaying results after conversion\n",
        "print(df_events_merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqPVZEbw1_ao"
      },
      "source": [
        "Now that types are the same, we can use the calculated field `next_expected_payment_date` to compare with `next_payment_day`from backend. In scope of this, I will create a new field called `is_next_payment_day_correct` with results TRUE or FALSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acbYN3Za2nLE"
      },
      "outputs": [],
      "source": [
        "# Creating new column with comparison results\n",
        "df_events_merged['is_next_payment_day_correct'] = (\n",
        "    df_events_merged['next_expected_payment_date'] == df_events_merged['next_payment_day']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J70emLOA2qkH"
      },
      "source": [
        "After adding this column, we now have information about whether the `next_expected_payment_date` matches `the next_payment_day` from the backend.\n",
        "\n",
        "If the result is `False`, it indicates that the payment dates are being calculated incorrectly. This could be due to various factors, which I will investigate further in the next steps.\n",
        "\n",
        "If the result is `True`, it could indicate two possibilities:\n",
        "\n",
        "\n",
        "1.   The `next_payment_day` is correct because the user was properly synchronized with backend\n",
        "2.   The `True` result was generated by coincidence, with no correlation to the correct timestamp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRLvlnZq5cAO"
      },
      "outputs": [],
      "source": [
        "#Displaying results for first visual analysis\n",
        "print(df_events_merged.head(30))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGigI0M75lrz"
      },
      "source": [
        "After presenting these results for a preliminary analysis, it becomes clear that the majority of cases return False, and this is correlated with the timestamp difference between the events table and the backend table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWXJVEMg6CMo"
      },
      "source": [
        "In this case, to avoid performing numerous calculations with a date field (which can easily lead to discrepancies), I realized, after doing a quick manual calculation, that in some cases the next_payment_date field is being calculated based on the timestamp from the backend table (instead of the events table). This leads to incorrect calculations because most of the events are not synchronized at the same time they enter the events table, so they are added to the backend table with some delays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ3Jaalg7UhY"
      },
      "outputs": [],
      "source": [
        "# Converting the columns to datetime format, in case they are not already in that format\n",
        "df_events_merged['event.timestamp'] = pd.to_datetime(df_events_merged['event.timestamp'])\n",
        "df_events_merged['updated_at'] = pd.to_datetime(df_events_merged['updated_at'])\n",
        "\n",
        "# Calculating the timestamp difference between the backend and events tables\n",
        "df_events_merged['timestamp_diff'] = df_events_merged['event.timestamp'] - df_events_merged['updated_at']\n",
        "\n",
        "# Displaying the result to verify the timestamp difference\n",
        "print(\"df_events_merged with new column timestamp_diff:\")\n",
        "print(df_events_merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWSsk-2oApzK"
      },
      "source": [
        "After calculating the time difference between the synchronization of the events table and the backend, and after performing a quick manual check, I noticed that in some cases, the next_payment_day in the backend table is calculated based on the timestamp from the backend (which may differ from the timestamp in the events table), leading to errors in the calculation of the next_payment_day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHHrd-ZMBpVB"
      },
      "source": [
        "To be certain, I decided to calculate a new field using the `calculate_incremented_date` function, where I will base the calculation on the updated_at field to determine the exact next payment day, without considering the loop that should be performed until today to identify the correct next payment day, as we did in the other function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L95WlpCBCaLG"
      },
      "outputs": [],
      "source": [
        "df_events_merged[\"next_payment_day_from_updated_at\"] = df_events_merged.apply(\n",
        "    lambda row: calculate_incremented_date(\n",
        "        row[\"updated_at\"],\n",
        "        row[\"frequency\"],\n",
        "        row[\"day\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Displaying new column next_payment_day_from_updated_at in dataframe:\n",
        "print(\"Displaying new column next_payment_day_from_updated_at in dataframe:\")\n",
        "print(df_events_merged[['user.id', 'creation_date', 'frequency', 'day', 'updated_at', 'next_payment_day']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeJ_xij_DGAY"
      },
      "source": [
        "Now, I will compare with `next_payment_day`from backend to ensure that my theory is right for such cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iySlOlUDnoB"
      },
      "outputs": [],
      "source": [
        "# Comparison between fields 'next_payment_day' from backend and new calculated field 'next_payment_day_from_updated_at'\n",
        "df_events_merged['match_with_updated_at'] = (\n",
        "    df_events_merged['next_payment_day'] == df_events_merged['next_payment_day_from_updated_at']\n",
        ")\n",
        "\n",
        "# Displaying new column next_payment_day_from_updated_at in dataframe:\n",
        "print(\"Displaying new column 'match_with_updated_at' in dataframe:\")\n",
        "print(df_events_merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIEr9pbQFD-N"
      },
      "source": [
        "From a superficial analysis of the printed results with `True` or `False`, it is evident that, in some cases, the `next_payment_day` is being calculated using the `updated_at field`. This field, in most cases, does not synchronize immediately with the user’s events table, leading to errors in the calculation of the `next_payment_date` and also says that the logic for this field in backend is not doing the looping to reach frequencies till current day in calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIT6K8ZuEyRH"
      },
      "source": [
        "# Analysis of Payment Date Discrepancies in `allowance_backend_table` : Identifying Patterns and Hypotheses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9kdzd_3Gmum"
      },
      "source": [
        "Now that we have calculated auxiliary fields such as match_with_updated_at, which allows us to identify errors and their types by comparing with next_payment_day, and next_expected_payment_date, which shows the actual date when the payment should occur, we can begin our quantitative analysis and report from backend team."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aus2acDoHtD9"
      },
      "source": [
        "### First analysis:\n",
        "\n",
        "*Filtering the rows where match_with_payment_next_day is True and is_next_payment_day_correct is also True.*\n",
        "\n",
        "These combined conditions mean that the code is filtering the records where both payment date validation criteria are true, that is, where the system correctly calculated the next payment day and this calculation matches the expected one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfplV8NxGLaR"
      },
      "outputs": [],
      "source": [
        "# Filtering\n",
        "filtered_rows = df_events_merged[(df_events_merged['match_with_updated_at'] == True) &\n",
        "                                   (df_events_merged['is_next_payment_day_correct'] == True)]\n",
        "\n",
        "# Calculating the number of filtered rows\n",
        "count_filtered_rows = len(filtered_rows)\n",
        "\n",
        "# Calculating the percentage relative to the total number of rows in the DataFrame\n",
        "percentage = (count_filtered_rows / len(df_events_merged)) * 100\n",
        "\n",
        "# Displaying the result of counting and percentage\n",
        "print(f\"Percentage of rows where match_with_updated_at and is_next_payment_day_correct are both TRUE: {percentage:.2f}%\")\n",
        "print(f\"Total number of records where both criteria are TRUE: {count_filtered_rows}\")\n",
        "\n",
        "# Display the rows where both conditions are TRUE\n",
        "print(\"Rows where both match_with_updated_at and is_next_payment_day_correct are TRUE:\")\n",
        "print(filtered_rows.head()) #adjust here if you want to see all filtered_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1vF6Q0YKywH"
      },
      "source": [
        "If the values in the calculated columns and the payment_next_day column match, it indicates that there are no discrepancies in the data. Therefore, we can exclude these records from the analysis moving forward, as they are considered correct and consistent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dzvE76iLHlX"
      },
      "outputs": [],
      "source": [
        "# Removing rows where 'match_with_updated_at' is True and 'is_next_payment_day_correct' is True\n",
        "df_events_adjusted = df_events_merged[~((df_events_merged['match_with_updated_at'] == True) &\n",
        "                                      (df_events_merged['is_next_payment_day_correct'] == True))]\n",
        "\n",
        "# Displaying the cleaned DataFrame\n",
        "print(\"Displaying dataframe without correct next_payment_day to focus on discrepancies:\")\n",
        "print(df_events_adjusted.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaTyg82FKRby"
      },
      "source": [
        "### Second analysis:\n",
        "\n",
        "Filtering the rows where `match_with_updated_at` is `True` and `is_next_payment_day_correct` is `False`.\n",
        "\n",
        "This means that the rows that match with match_with_updated_at but do not match with `is_next_payment_day_correct` are taking into account the `updated_at` timestamp to calculate the `next_payment_day`. This indicates that either the logic of the loop to calculate the nextpayment day, considering the current date, is inconsistent, or that the `updated_at` timestamp has a delay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3mwavUhNm0G"
      },
      "outputs": [],
      "source": [
        "# Filtering the rows where match_with_payment_next_day is True and is_next_payment_day_correct is False\n",
        "filtered_rows_2 = df_events_merged[(df_events_merged['match_with_updated_at'] == True) &\n",
        "                                   (df_events_merged['is_next_payment_day_correct'] == False)]\n",
        "\n",
        "# Calculating the number of filtered rows\n",
        "count_filtered_rows_2 = len(filtered_rows_2)\n",
        "\n",
        "# Calculating the percentage relative to the total number of rows in the DataFrame\n",
        "percentage = (count_filtered_rows_2 / len(df_events_merged)) * 100\n",
        "\n",
        "# Display the rows where both conditions are TRUE\n",
        "print(\"Rows where both match_with_updated_at and is_next_payment_day_correct are TRUE:\")\n",
        "print(filtered_rows_2.head()) #adjust here if you want to see all filtered_rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkCiUmeJO3oU"
      },
      "source": [
        "If the values in the calculated columns and the payment_next_day column match, it indicates that there are discrepancies in the data. Both caused by delays in sync with events table or because the logic is failing to calculate the next_payment_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq9-lgJjRZWT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Adding reason_of_discrepancy only to rows in filtered_rows_2\n",
        "df_events_adjusted['reason_of_discrepancy'] = df_events_adjusted.apply(\n",
        "    lambda row: categorize_discrepancy(row) if row.name in filtered_rows_2.index else '', axis=1)\n",
        "\n",
        "# Count the number of rows for each category\n",
        "category_counts = df_events_adjusted['reason_of_discrepancy'].value_counts()\n",
        "\n",
        "# Print the counts for each category\n",
        "print(\"Count of rows categorized by reason_of_discrepancy:\")\n",
        "print(category_counts)\n",
        "\n",
        "# Print rows for each category\n",
        "for category in category_counts.index:\n",
        "    print(f\"\\nRows categorized as '{category}':\")\n",
        "    print(df_events_adjusted[df_events_adjusted['reason_of_discrepancy'] == category][['user.id', 'reason_of_discrepancy']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDAamvxQSBjq"
      },
      "source": [
        "# Backend Analysis Report\n",
        "After categorizing the discrepancies based on the timestamp differences and analyzing the reason_of_discrepancy, we observe the following:\n",
        "\n",
        "Backend Logic Issues: The discrepancies categorized under \"backend logic issues\" indicate that the timestamp difference is less than one day, which suggests potential issues with how the backend is handling date logic.\n",
        "\n",
        "Timestamp Delay Issues: On the other hand, discrepancies under the \"timestamp delay issue\" category indicate that the issue is related to a delay in the updated_at timestamp, which could be affecting the payment day calculations.\n",
        "\n",
        "\n",
        "1.   **Backend Logic Issues:** The discrepancies categorized under \"backend logic issues\" indicate that the timestamp difference is less than one day, which suggests potential issues with how the backend is handling date logic\n",
        "2.   **Timestamp Delay Issues:** On the other hand, discrepancies under the \"timestamp delay issue\" category indicate that the issue is related to a delay in the updated_at timestamp, which could be affecting the payment day calculations\n",
        "\n",
        "\n",
        "**25 Rows Reporting Unknown Issues**: Among the categorized rows, we have 25 entries that are categorized as having an unknown issue. This discrepancy needs to be further investigated to determine its root cause and resolve it effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ONHAGNoSyq0"
      },
      "outputs": [],
      "source": [
        "# Saving the final dataframe with the new 'reason_of_discrepancy' column to a CSV file\n",
        "output_file = 'discrepancies_in_payment_dates.csv'\n",
        "df_events_adjusted.to_csv(output_file, index=False)\n",
        "\n",
        "# Confirming the file is saved\n",
        "print(f\"\\nThe final DataFrame has been saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EaoN46zUqYi"
      },
      "source": [
        "There are two users who do not have a UUID created in the Backend table\n",
        "\n",
        "1.   1cf825ad-c6fc-4881-9df1-20b495f375d8\n",
        "2.   f0c58b79-2e41-4487-970f-fe6206bbe20b\n",
        "\n",
        "And also we can find an extra user User.Id/UUID = X, clearly indicating an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbB3naNVXUS"
      },
      "source": [
        "# Payment Schedule Backend Table analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHhlO9ccgtyO"
      },
      "source": [
        "In addition to removing duplicates from the `user_id` field, the analysis also checks if the `payment_date` field from the `df_payment` table matches the `next_payment_day` field from the backend table and also `next_expected_payment_date`from events. In other words, it compares the payment dates between the three tables to see if they align for each user.\n",
        "\n",
        "Essentially, I am performing two analyses:\n",
        "\n",
        "Removing duplicates: Ensuring that each user_id in the df_payment table is unique.\n",
        "Verifying date consistency: Validating if the payment dates in df_payment are correct according to the information from the backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj97lE_WVk87"
      },
      "outputs": [],
      "source": [
        "# Counting how many times each user_id appears\n",
        "user_id_counts = df_payment['user_id'].value_counts()\n",
        "\n",
        "# Filtering only the user_ids that appear more than once\n",
        "duplicate_user_ids = user_id_counts[user_id_counts > 1]\n",
        "\n",
        "print(f\"Number of duplicate user_ids: {len(duplicate_user_ids)}\")\n",
        "print(duplicate_user_ids)\n",
        "\n",
        "# Removing duplicates and keeping only the first occurrence of each user_id\n",
        "df_payment_unique = df_payment.drop_duplicates(subset='user_id', keep='first')\n",
        "\n",
        "# Verifying the result\n",
        "print(f\"Dataframe after removing duplicates: {df_payment_unique.head()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcPoyX2enUCf"
      },
      "source": [
        "As next step, I bring again the table `df_events_merged`to compare datas between fields `payment_date`(df_payment), `next_payment_day`(backend) and `next_expected_payment_date`(events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akfw_rHxhey3"
      },
      "outputs": [],
      "source": [
        "df_final_merged = pd.merge(\n",
        "    df_events_merged,  # DataFrame df_merged (with backend and user_id links)\n",
        "    df_payment,        # DataFrame df_payment\n",
        "    left_on='user.id', # Key from df_merged\n",
        "    right_on='user_id',# Key from df_payment\n",
        "    how='left',        # Left join to retain all records from df_merged\n",
        "    suffixes=('_merged', '_payment')  # Suffixes for column names after merge\n",
        ")\n",
        "\n",
        "# Displaying the resulting DataFrame to verify the merge outcome\n",
        "print(df_final_merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPuWOLmKnt6F"
      },
      "source": [
        "The purpose of bringing in these fields aligns with the following logic:\n",
        "\n",
        "1. **'backend error - logic'**: When payment_date (df_payment) matches next_payment_day(backend) but does not match next_expected_payment_date(events), indicating a logic error, but means that the date is sync with backend.\n",
        "2. **'backend error - timestamp'**: When payment_date matches next_expected_payment_date but does not match next_payment_day, suggesting that the logic is correct with events\n",
        "3. **'unknown error'**: When payment_date matches neither next_payment_day nor next_expected_payment_date, signaling an unknown error or discrepancy.\n",
        "4. **'correct payment date'**: When payment_date matches both next_payment_day and next_expected_payment_date, indicating the backend logic and timestamp are correct for payment.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruNk2LqHnzsU"
      },
      "outputs": [],
      "source": [
        "# Creating a new column to categorize the payment date matches\n",
        "df_final_merged['payment_date_status'] = df_final_merged.apply(\n",
        "    lambda row: 'backend error - logic' if row['payment_date'] == row['next_payment_day'] and row['payment_date'] != row['next_expected_payment_date'] else\n",
        "               ('backend error - timestamp' if row['payment_date'] != row['next_payment_day'] and row['payment_date'] == row['next_expected_payment_date'] else\n",
        "               ('unknown error' if row['payment_date'] != row['next_payment_day'] and row['payment_date'] != row['next_expected_payment_date'] else\n",
        "               'correct payment date')), axis=1)\n",
        "\n",
        "# Display the result\n",
        "print(df_final_merged[['user_id', 'payment_date', 'next_payment_day', 'next_expected_payment_date', 'payment_date_status']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaLOsFKlo8T7"
      },
      "source": [
        "As the final result, I export in CSV file for backend team analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv5PIGmOpFA0"
      },
      "outputs": [],
      "source": [
        "# Save the result to a CSV file\n",
        "output_file_payment_status = 'payment_table_discrepancy.csv'\n",
        "df_final_merged.to_csv(output_file_payment_status, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
